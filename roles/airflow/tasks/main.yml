---

- name: ensure airflow user exists
  user:
    name: airflow
    comment: "Airflow user"
    shell: /bin/bash
  tags: airflow

- name: ensure airflow user has authorized_keys set
  authorized_key: user=airflow key="{{ lookup('file', 'public_keys/' + item + '_id_rsa.pub') }}"
  with_items: "{{ ['hadoop'] + airflow_admins }}"
  tags: airflow

- name: assert hdfs /user/airflow dir exists
  shell: /usr/local/hadoop-2.9.2/bin/hdfs dfs -ls /user/airflow
  register: assert_user_airflow_dir_exists
  changed_when: no
  ignore_errors: yes
  tags: airflow

- name: create hdfs /user/airflow dir if it doesn't exist
  shell: /usr/local/hadoop-2.9.2/bin/hdfs dfs -mkdir -p /user/airflow &&
         /usr/local/hadoop-2.9.2/bin/hdfs dfs -chown airflow:supergroup /user/airflow
  when: assert_user_airflow_dir_exists is failed
  become: yes
  become_user: hadoop
  tags: airflow

- name: ensure better bash history for airflow user
  blockinfile:
    dest: /home/airflow/.bashrc
    mode: 0644
    marker: "# {mark} ANSIBLE MANAGED BLOCK airflow-history"
    block: |
      HISTFILESIZE=20000
      HISTSIZE=10000
      HISTTIMEFORMAT='%y-%m-%dT%T  '
  become: yes
  become_user: airflow
  tags: airflow

- name: ensure airflow ~/airflow_home and ~/o3 dirs exists
  file:
    path: /home/airflow/{{ item }}
    state: directory
  with_items:
    - airflow_home
    - o3
  become: yes
  become_user: airflow
  tags: airflow

- name: ensure airflow config matches template
  template:
    src: airflow.cfg.j2
    dest: /home/airflow/airflow_home/airflow.cfg
  become: yes
  become_user: airflow
  notify:
    - airflow initdb
    - restart airflow-webserver
    - restart airflow-scheduler
  tags: airflow

- name: ensure latest code base is copied to remote server
  copy:
    src: '{{ item }}'
    dest: /home/airflow/o3/{{ item }}
  become: yes
  become_user: airflow
  with_items:
    - aac.py
    - setup.py
    - o3/__init__.py
    - o3/constants.py
    - o3/utils.py
    - o3/dags/__init__.py
    - o3/dags/dag1.py
    - o3/dags/dag2.py
    - o3/dags/dag3.py
    - o3/hooks/__init__.py
    - o3/hooks/hdfs_hook.py
    - o3/hooks/pyhive_hook.py
    - o3/operators/__init__.py
    - o3/operators/convert_log_to_avro_operator.py
    - o3/operators/ensure_dir_operator.py
    - o3/operators/filter_logs_to_percentage_operator.py
    - o3/operators/ingest_avro_into_hive_operator.py
    - o3/operators/move_file_operator.py
    - o3/operators/remove_file_operator.py
    - o3/operators/row_count_operator.py
    - o3/operators/split_log_by_classifiers_operator.py
    - o3/operators/word_count_operator.py
    - environment-linux.yml
  tags: airflow

- name: ensure anaconda3 environment is set up
  include_role:
    name: anaconda3
  vars:
    anaconda3_user: airflow
    anaconda3_environment_file: /home/airflow/o3/environment-linux.yml
  tags: airflow

- name: ensure o3 package is installed within the o3 conda environment
  shell: bash -il -c 'source /home/airflow/anaconda3/bin/activate o3 &&
         pip install -e /home/airflow/o3'
  become: yes
  become_user: airflow
  register: o3_package_installed
  changed_when: "'Found existing installation: o3' not in o3_package_installed.stdout"
  notify:
    - airflow initdb
    - restart airflow-webserver
    - restart airflow-scheduler
  tags: airflow

- name: ensure airflow-enviroment matches template
  template:
    src: environment.j2
    dest: /home/airflow/airflow-environment
  become: yes
  become_user: airflow
  notify:
    - restart airflow-webserver
    - restart airflow-scheduler
  tags: airflow

- name: ensure airflow-environment is sourced in {{ bash_profile_file }}
  blockinfile:
    path: /home/airflow/{{ bash_profile_file }}
    marker: "# {mark} ANSIBLE MANAGED BLOCK airflow-environment"
    block: |
      set -a
      source /home/airflow/airflow-environment
      export PATH=/usr/local/hadoop-2.9.2/bin:/usr/local/hadoop-2.9.2/sbin:$PATH
      export JAVA_HOME={{ java_home }}
  become: yes
  become_user: airflow
  tags: airflow

- name: ensure postgres database is started
  docker_container:
    name: airflow_postgres
    image: postgres:9.6
    state: started
    restart_policy: unless-stopped
    volume_driver: local
    volumes:
      - airflow_postgres:/var/lib/postgresql/data
    ports:
      - 5432:5432
    env:
      POSTGRES_DB: airflow
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: "{{ airflow_db_user_password }}"
  tags: airflow

- name: ensure airflow services matches template
  template:
    src: airflow-{{ item }}.service.j2
    dest: /etc/systemd/system/airflow-{{ item }}.service
  with_items:
    - webserver
    - scheduler
  register: add_airflow_services
  notify:
    - restart airflow-webserver
    - restart airflow-scheduler
  tags: airflow

- name: ensure daemon-reload is run on airflow service modification
  shell: systemctl daemon-reload
  when: add_airflow_services is changed
  tags: airflow

- name: ensure airflow services is running and auto-starting
  service:
    name: airflow-{{ item }}
    state: started
    enabled: yes
  with_items:
    - webserver
    - scheduler
  tags: airflow

...
